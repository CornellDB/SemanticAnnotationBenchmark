{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "from langchain import PromptTemplate, LLMChain, OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env file with API KEY using full path\n",
    "config = dotenv_values(\".env\")\n",
    "os.environ['OPENAI_API_KEY'] = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b93f4d",
   "metadata": {},
   "source": [
    "## Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "with open(\"c2_data/column_gt.txt\") as f:\n",
    "    column_gt = defaultdict(dict)\n",
    "    for line in f.readlines():\n",
    "        table_name_col, concepts = line.strip('\\n').split(',', 1)\n",
    "        concepts = [concept.strip('\"') for concept in concepts.split(\",\")]\n",
    "        table_name, col_idx = table_name_col.strip('\"').split(\" \")\n",
    "        column_gt[table_name][col_idx] = concepts\n",
    "print(column_gt)\n",
    "\n",
    "with open(\"c2_data/table_names.txt\") as f:\n",
    "    table_names = [line.strip('\\n').strip('\"') for line in f.readlines()]\n",
    "\n",
    "labels = []\n",
    "col_idxs = []\n",
    "examples_top_rows = []\n",
    "full_examples = []\n",
    "for table_name in table_names:\n",
    "    with open(f\"c2_data/tables/{table_name}\") as f:\n",
    "        lines = list(f.readlines())\n",
    "        lines = [line.strip(\"\\n\").replace(\"\\t\", \" || \") for line in lines]\n",
    "        columns_count = len(lines[0].split(\" || \"))\n",
    "        columns_row_count = len(lines[1].split(\" || \"))\n",
    "        if columns_row_count != columns_count:\n",
    "            if columns_row_count < columns_count:\n",
    "                for i in range(1, len(lines)):\n",
    "                    lines[i] += \" || \" * (columns_count - columns_row_count)\n",
    "            elif columns_row_count > columns_count:\n",
    "                for i in range(1, len(lines)):\n",
    "                    current = lines[i].split(\" || \")\n",
    "                    lines[i] = \" || \".join(current[:columns_count])\n",
    "        poss_labels = []\n",
    "        table_col_idx = []\n",
    "        if table_name in column_gt:\n",
    "            for idx, concepts in column_gt[table_name].items():\n",
    "                poss_labels.append(concepts[0])\n",
    "                table_col_idx.append(idx)\n",
    "        labels.append(poss_labels)\n",
    "        col_idxs.append(table_col_idx)\n",
    "        new_column_header = \" || \".join([f\"Column {i}\" for i in range(columns_count)]) + \"\\n\"\n",
    "        examples_top_rows.append(new_column_header + \"\\n\".join(lines[1:6]))\n",
    "        full_examples.append(new_column_header + \"\\n\".join(lines[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155be31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = 88\n",
    "print(table_names[table_id])\n",
    "print(examples_top_rows[table_id])\n",
    "print(col_idxs[table_id])\n",
    "print(labels[table_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33eeece",
   "metadata": {},
   "source": [
    "## Choose prompt template: without or with instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper name: table\n",
    "original_template = \"\"\"\n",
    "\n",
    "Answer the question based on the task below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "\n",
    "Task: Classify the columns of a given table with only one of the following classes that are separated with comma: description of event, description of restaurant, locality of address, postal code, region of address, country, price range, telephone, date, name of restaurant, payment accepted, day of week, review, organization, date and time, coordinate, name of event, event attendance mode, event status, currency, time, description of hotel, name of hotel, location feature, rating, fax number, email, photograph, name of music recording, music artist, name of album, duration.\n",
    "\n",
    "Table: {input}\n",
    "\n",
    "Class:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Paper name: table + instructions\n",
    "original_inst_template = \"\"\"\n",
    "\n",
    "Answer the question based on the task and instructions below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "\n",
    "Task: Classify the columns of a given table with only one of the following classes that are separated with comma: description of event, description of restaurant, locality of address, postal code, region of address, country, price range, telephone, date, name of restaurant, payment accepted, day of week, review, organization, date and time, coordinate, name of event, event attendance mode, event status, currency, time, description of hotel, name of hotel, location feature, rating, fax number, email, photograph, name of music recording, music artist, name of album, duration.\n",
    "\n",
    "Instructions: 1. Look at the input given to you and make a table out of it. 2. Look at the cell values in detail. 3. For each column, select a class that best represents the meaning of all cells in the column. 4. Answer with the selected class for each columns with the format Column1: class.\n",
    "\n",
    "Table:\n",
    "{input}\n",
    "\n",
    "Class:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New prompt\n",
    "semantic_concept_template = \"\"\"\n",
    "\n",
    "Answer the question based on the task below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "\n",
    "Task: Suggest a semantic concept for each column of a given table. Answer with the semantic concept for each column with the format Column1: semantic concept.\n",
    "\n",
    "Table: {input}\n",
    "\n",
    "Semantic concepts:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper name: table + instructions\n",
    "inst_template = \"\"\"\n",
    "\n",
    "Answer the question based on the task and instructions below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "\n",
    "Task: Suggest a semantic concept for each column of a given table.\n",
    "\n",
    "Instructions: 1. Look at the input given to you and make a table out of it. 2. Look at the cell values in detail. 3. For each column, suggest a semantic concept that best represents the meaning of all cells in the column. 4. Answer with the semantic concept for each column with the format Column1: semantic concept.\n",
    "\n",
    "Table:\n",
    "{input}\n",
    "\n",
    "Semantic concepts:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69280f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt to ask gpt to classify labels \n",
    "classify_label_template = \"\"\"\n",
    "\n",
    "Task: Classify the semantic concept {input} with only one of the following classes that are separated with comma: description of event, description of restaurant, locality of address, postal code, region of address, country, price range, telephone, date, name of restaurant, payment accepted, day of week, review, organization, date and time, coordinate, name of event, event attendance mode, event status, currency, time, description of hotel, name of hotel, location feature, rating, fax number, email, photograph, name of music recording, music artist, name of album, duration.\n",
    "\n",
    "Class:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cc367",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_template = \"\"\"\n",
    "\n",
    "Answer the question based on the task below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "\n",
    "Task: Suggest a semantic concept for each column of a given table. Answer with the semantic concept for each column with the format Column1: semantic concept.\n",
    "\n",
    "Table: {input}\n",
    "\n",
    "Semantic concepts:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "debug_template_top_5 = \"\"\"\n",
    "\n",
    "Answer the question based on the task below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "\n",
    "Task: Suggest 5 possible semantic concept for each column of a given table. Answer with the semantic concept for each column with the format Column1: possible semantic concept 1, possible semantic concept 2, possible semantic concept 3, possible semantic concept 4, possible semantic concept 5. \n",
    "\n",
    "Table: {input}\n",
    "\n",
    "Semantic concepts:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "debug_template_album = \"\"\"\n",
    "\n",
    "Could this column be about music albums?\n",
    "\n",
    "Table: {input}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "check_template = \"\"\"\n",
    "\n",
    "Critique whether these semantic concepts matches their respective columns in the given table and improve on them. If there is no further improvements to be made, just say 'It's good'.\n",
    "\n",
    "Semantic concepts: {prev_output}\n",
    "\n",
    "Table: {input}\n",
    "\n",
    "Answer with the semantic concept for each column with the format Column1: semantic concept. \n",
    "\n",
    "Semantic concepts:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "check_template_best_of_5 = \"\"\"\n",
    "\n",
    "Critique whether these semantic concepts matches their respective columns in the given table and improve on them. Choose the best semantic concept for each column.\n",
    "\n",
    "Semantic concepts: {prev_output}\n",
    "\n",
    "Table: {input}\n",
    "\n",
    "Answer with the semantic concept for each column with the format Column1: semantic concept. \n",
    "\n",
    "Semantic concepts:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19177971",
   "metadata": {},
   "source": [
    "## Load LLM and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940af79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpt_3_turbo = ChatOpenAI(model_name='gpt-3.5-turbo-0301', temperature=0)\n",
    "gpt_4 = ChatOpenAI(model_name='gpt-4-0613', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca67e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type = \"semantic_concept\"\n",
    "if prompt_type == \"original_template\":\n",
    "    prompt = PromptTemplate(template=original_template, input_variables=['input'])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=gpt_3_turbo)\n",
    "    llm_chain_4 = LLMChain(prompt=prompt, llm=gpt_4)\n",
    "elif prompt_type == \"semantic_concept\":\n",
    "    prompt = PromptTemplate(template=semantic_concept_template, input_variables=['input'])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=gpt_3_turbo)\n",
    "    llm_chain_4 = LLMChain(prompt=prompt, llm=gpt_4)\n",
    "elif prompt_type == \"with_inst\":\n",
    "    prompt = PromptTemplate(template=inst_template, input_variables=['input'])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=gpt_3_turbo)\n",
    "    llm_chain_4 = LLMChain(prompt=prompt, llm=gpt_4)\n",
    "elif prompt_type == \"debug_template\":\n",
    "    prompt = PromptTemplate(template=debug_template, input_variables=['input'])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=gpt_3_turbo)\n",
    "    llm_chain_4 = LLMChain(prompt=prompt, llm=gpt_4)\n",
    "elif prompt_type == \"debug_template_top_5\":\n",
    "    prompt = PromptTemplate(template=debug_template_top_5, input_variables=['input'])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=gpt_3_turbo)\n",
    "    llm_chain_4 = LLMChain(prompt=prompt, llm=gpt_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_column_major(example: str) -> str:\n",
    "    lines = example.split(\"\\n\")\n",
    "    col_major = [col.strip() + \": \" for col in lines[0].split(\"||\")]\n",
    "    for line in lines[1:]:\n",
    "        for i, val in enumerate(line.split(\"||\")):\n",
    "            col_major[i] += val + \", \"\n",
    "    debug_eg = \"\\n\".join(col_major) # Not needed to remove last row like when handling sotab\n",
    "    return debug_eg\n",
    "\n",
    "print(convert_to_column_major(examples_top_rows[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preds(preds: list[str], file_name: str):\n",
    "    #Save predictions in a file:\n",
    "    with open(file_name,'wb') as f:\n",
    "        pickle.dump(preds,f)\n",
    "\n",
    "def load_preds(file_name: str):\n",
    "    #Save predictions in a file:\n",
    "    with open(file_name,'rb') as f:\n",
    "        preds = pickle.load(f)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e985602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero-shot prediction\n",
    "preds_gpt_4 = []\n",
    "for example in tqdm(examples_top_rows):\n",
    "    preds_gpt_4.append(llm_chain_4.run({'input': example}))\n",
    "save_preds(preds_gpt_4, \"predictions_c2/gpt4-prompt-table-without-instructions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero-shot prediction\n",
    "preds_gpt_4 = []\n",
    "for example in tqdm(examples_top_rows):\n",
    "    preds_gpt_4.append(llm_chain_4.run({'input': convert_to_column_major(example)}))\n",
    "save_preds(preds_gpt_4, \"predictions_c2/gpt4-prompt-table-without-instructions-col-major.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59acb9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = load_preds(\"predictions_c2/gpt4-prompt-table-without-instructions.pkl\")\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = load_preds(\"predictions_c2/gpt4-prompt-table-without-instructions-col-major.pkl\")\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"raw_prompt_output\": preds})\n",
    "df.to_csv(\"predictions/gpt4-prompt-table-without-instructions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6cab8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003621b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_new(preds, col_idxs, bert_threshold=0.85):\n",
    "    ids, predictions, original_preds, highest_bertscores = [], [], [], []\n",
    "    for j, table_preds in enumerate(tqdm(preds)):\n",
    "        if \"Semantic concepts:\" in table_preds:\n",
    "            table_preds = table_preds.split(\"Class:\")[1]\n",
    "        \n",
    "        #Break predictions into either \\n or ,\n",
    "        if \":\" in table_preds:\n",
    "            separator = \":\"\n",
    "        elif \"-\" in table_preds:\n",
    "            separator = \"-\"  \n",
    "        else:\n",
    "            separator = \",\"\n",
    "            \n",
    "        col_preds = table_preds.split(separator)\n",
    "        i=0\n",
    "\n",
    "        table_name = table_names[j]\n",
    "        columns_to_eval = column_gt[table_name]\n",
    "        for col_idx in col_idxs[j]:\n",
    "            # print(idx, gt)\n",
    "            gt = columns_to_eval[col_idx]\n",
    "            ids.append(j)\n",
    "            original_preds.append(table_preds)\n",
    "            if int(col_idx) >= len(col_preds):\n",
    "                predictions.append('-')\n",
    "                highest_bertscores.append(0)\n",
    "            else:\n",
    "                pred = col_preds[int(col_idx)]\n",
    "                # Remove break lines\n",
    "                if \"\\n\" in pred:\n",
    "                    pred = pred.split('\\n')[0].strip()\n",
    "                # Remove commas\n",
    "                if \",\" in pred:\n",
    "                    pred = pred.split(\",\")[0].strip()\n",
    "                # Remove paranthesis\n",
    "                if '(' in pred:\n",
    "                    pred = pred.split(\"(\")[0].strip()\n",
    "                #Remove points\n",
    "                if '.' in pred:\n",
    "                    pred = pred.split(\".\")[0].strip()\n",
    "                # Lower-case prediction\n",
    "                pred = pred.strip().lower()\n",
    "                # print(pred, gt)\n",
    "                bertscores = np.array(bertscore.compute(predictions=[pred]*len(gt), references=gt, lang=\"en\")[\"f1\"])\n",
    "                highest_score = max(bertscores)\n",
    "                highest_bertscores.append(highest_score)\n",
    "\n",
    "                if highest_score > bert_threshold:\n",
    "                    predictions.append(gt[0])\n",
    "                else:\n",
    "                    print(f\"For test example {j} out of label space prediction: {pred}, true label: {gt}\")\n",
    "                    predictions.append('-')\n",
    "            i+=1\n",
    "            \n",
    "    return ids, predictions, original_preds, highest_bertscores\n",
    "\n",
    "ids, class_predictions, original_preds, highest_bertscores = evaluation_new(preds, col_idxs, bert_threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748dfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids), len(class_predictions), len(original_preds), len(highest_bertscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_labels = [label for table in labels for label in table]\n",
    "flattened_col_idxs = [idx for table in col_idxs for idx in table]\n",
    "df = pd.DataFrame({\"prompt_output_id\": ids, \n",
    "                   \"col_idx\": flattened_col_idxs,\n",
    "                   \"label\": flattened_labels, \n",
    "                   \"original_pred\": original_preds, \n",
    "                   \"class_pred_using_bert\": class_predictions, \n",
    "                   \"highest_bertscore\": highest_bertscores})\n",
    "df.to_csv(\"predictions_c2/preds_gpt4_without_inst_col_major.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2350d",
   "metadata": {},
   "source": [
    "### Calculate Precision, Recall, Macro-F1 and Micro-F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81907f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"predictions_c2/preds_gpt4_without_inst_col_major.csv\",index_col=0)\n",
    "eval_labels, eval_preds = df[\"label\"], df[\"class_pred_using_bert\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80edaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df[df[\"label\"] == df[\"class_pred_using_bert\"]]))\n",
    "df['label_in_top_5'] = df[['label','top_5_preds']].apply(\n",
    "    lambda row: row['label'] in row['top_5_preds'], axis=1\n",
    ")\n",
    "print(len(df[df[\"label_in_top_5\"] == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a1bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_scores(y_tests, y_preds):\n",
    "    types = list(set(y_tests))\n",
    "    types = types + [\"-\"]\n",
    "    num_classes = len(types)\n",
    "    \n",
    "    y_tests = [types.index(y) for y in y_tests]\n",
    "    y_preds = [types.index(y) for y in y_preds]\n",
    "    \n",
    "    #Confusion matrix\n",
    "    cm = np.zeros(shape=(num_classes,num_classes))\n",
    "    \n",
    "    for i in range(len(y_tests)):\n",
    "        cm[y_preds[i]][y_tests[i]] += 1\n",
    "        \n",
    "    report = {}\n",
    "    \n",
    "    for j in range(len(cm[0])):\n",
    "        report[j] = {}\n",
    "        report[j]['FN'] = 0\n",
    "        report[j]['FP'] = 0\n",
    "        report[j]['TP'] = cm[j][j]\n",
    "\n",
    "        for i in range(len(cm)):\n",
    "            if i != j:\n",
    "                report[j]['FN'] += cm[i][j]\n",
    "        for k in range(len(cm[0])):\n",
    "            if k != j:\n",
    "                report[j]['FP'] += cm[j][k]\n",
    "\n",
    "        precision = report[j]['TP'] / (report[j]['TP'] + report[j]['FP'])\n",
    "        recall = report[j]['TP'] / (report[j]['TP'] + report[j]['FN'])\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "        \n",
    "        if np.isnan(f1):\n",
    "            f1 = 0\n",
    "        if np.isnan(precision):\n",
    "            f1 = 0\n",
    "        if np.isnan(recall):\n",
    "            f1 = 0\n",
    "\n",
    "        report[j]['p'] =  precision\n",
    "        report[j]['r'] =  recall\n",
    "        report[j]['f1'] = f1\n",
    "    \n",
    "    all_fn = 0\n",
    "    all_tp = 0\n",
    "    all_fp = 0\n",
    "\n",
    "    for r in report:\n",
    "        if r != num_classes-1:\n",
    "            all_fn += report[r]['FN']\n",
    "            all_tp += report[r]['TP']\n",
    "            all_fp += report[r]['FP']\n",
    "        \n",
    "    class_f1s = [ report[class_]['f1'] for class_ in report]\n",
    "    class_p = [ 0 if np.isnan(report[class_]['p']) else report[class_]['p'] for class_ in report]\n",
    "    class_r = [ 0 if np.isnan(report[class_]['r']) else report[class_]['r'] for class_ in report]\n",
    "    macro_f1 = sum(class_f1s[:-1]) / (num_classes-1)\n",
    "    \n",
    "    p =  sum(class_p[:-1]) / (num_classes-1)\n",
    "    r =  sum(class_r[:-1]) / (num_classes-1)\n",
    "    micro_f1 = all_tp / ( all_tp + (1/2 * (all_fp + all_fn) )) \n",
    "    \n",
    "    per_class_eval = {}\n",
    "    for index, t in enumerate(types[:-1]):\n",
    "        per_class_eval[t] = {\"Precision\":class_p[index], \"Recall\": class_r[index], \"F1\": class_f1s[index]}\n",
    "    \n",
    "    evaluation = {\n",
    "        \"Micro-F1\": micro_f1,\n",
    "        \"Macro-F1\": macro_f1,\n",
    "        \"Precision\": p,\n",
    "        \"Recall\": r\n",
    "    }\n",
    "    \n",
    "    return evaluation, per_class_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1770b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation, per_class_eval = calculate_f1_scores(eval_labels, eval_preds)\n",
    "print(evaluation)\n",
    "print(per_class_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd645fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(labels, preds, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ba678",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"predictions/preds_gpt35_without_inst.csv\",index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = df[df[\"lionel_annot\"] != df[\"label\"]]\n",
    "error_df[\"table\"] = error_df.apply(lambda row: examples[row[\"prompt_output_id\"]], axis=1)\n",
    "error_df[\"all_labels\"] = error_df.apply(lambda row: test[row[\"prompt_output_id\"]][2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.reset_index(inplace=True, drop=True)\n",
    "error_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead582c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(error_df)):\n",
    "    print(\"=\"*10)\n",
    "    print(f\"Table {error_df.loc[idx, 'prompt_output_id']}\")\n",
    "    print(error_df.loc[idx,\"table\"])\n",
    "    print(error_df.loc[idx,\"all_labels\"])\n",
    "    \n",
    "    print(f\"Ground truth: {error_df.loc[idx,'label']}\")\n",
    "    print(f\"Raw output: {error_df.loc[idx,'parsed_col_pred']}\")\n",
    "    print(f\"Annot: {error_df.loc[idx,'lionel_annot']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af914f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"-\" means the model replied with out of label or with I don't know\n",
    "errors = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] != labels[i]:\n",
    "        errors += 1\n",
    "        print(f\"Predicted as {predictions[i]} when it was {labels[i]}\")\n",
    "errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
